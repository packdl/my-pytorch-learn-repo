{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN - Dense layers\n",
    "CNN - Convolutional neural network\n",
    "\n",
    "# 1-1 - \n",
    "## rank of input x from 0 to 2pi. \n",
    "Do not need to set the # of parameters equal, just close.\n",
    "\n",
    "First figure is training loss\n",
    "2nd figure is prediction of each model\n",
    "\n",
    "Import MNIST or CIFAR-10 from Torch vision. Use either CNN or DNN. Show loss and accuracy. Comment on results & show observservations\n",
    "\n",
    "# 1-2\n",
    "- Collect parameters of the models - Grab from every layer - Collect and put into 1 dimensional feaure than reduce to a feature with only 2 numbers\n",
    "- Look up PCA\n",
    "- FIgiure out how to compute second order optimization method ex: Newton's method or Levenberg-Marquardt algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "f'{device} is available'\n",
    "\n",
    "dtype = torch.float\n",
    "torch.set_default_device(device)\n",
    "\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        y_pred = model(X)\n",
    "        y_pred = y_pred.unsqueeze(1)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "    with torch.no_grad():\n",
    "        if epoch % 100 == 0:\n",
    "            print(epoch, loss.item())\n",
    "\n",
    "def val_loop(dataloader, model, loss_fn, epoch):\n",
    "    model.eval()\n",
    "    size= len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0,0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            y_pred = model(X)\n",
    "            y_pred = y_pred.unsqueeze(1)\n",
    "            test_loss += loss_fn(y_pred, y).item()\n",
    "            correct +=(y_pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /=size\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        display(f\"Accuracy: {(100*correct)}%, Avg loss: {test_loss} \\n\")\n",
    "\n",
    "    return test_loss\n",
    "    \n",
    "def cnt_model_params(model):\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            count+=param.numel()\n",
    "    return count\n",
    "\n",
    "def display_model_info(model_name, model):\n",
    "    count = 0\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Module):\n",
    "            count+=1\n",
    "    display(model)\n",
    "    display(f\"{model_name}. layers: {count}, parameters: {cnt_model_params(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CosNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=12, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=12, out_features=1, bias=True)\n",
       "    (5): Flatten(start_dim=0, end_dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'cos_model1. layers: 9, parameters: 1817'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CosNetwork2(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=453, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=453, out_features=2, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=2, out_features=1, bias=True)\n",
       "    (5): Flatten(start_dim=0, end_dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'cos_model2. layers: 9, parameters: 1817'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CosNetwork3(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=20, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=20, out_features=14, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=14, out_features=12, bias=True)\n",
       "    (9): ReLU()\n",
       "    (10): Linear(in_features=12, out_features=10, bias=True)\n",
       "    (11): ReLU()\n",
       "    (12): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (15): ReLU()\n",
       "    (16): Linear(in_features=10, out_features=9, bias=True)\n",
       "    (17): ReLU()\n",
       "    (18): Linear(in_features=9, out_features=1, bias=True)\n",
       "    (19): ReLU()\n",
       "    (20): Linear(in_features=1, out_features=1, bias=True)\n",
       "    (21): Flatten(start_dim=0, end_dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'cos_model3. layers: 25, parameters: 1815'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CosNetwork(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.linear_relu_stack = nn.Sequential(\n",
    "                nn.Linear(1, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 12),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(12, 1),\n",
    "                torch.nn.Flatten(0,1)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "              logits = self.linear_relu_stack(x)\n",
    "              return logits\n",
    "        \n",
    "class CosNetwork2(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.linear_relu_stack = nn.Sequential(\n",
    "                nn.Linear(1, 453),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(453, 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(2,1),\n",
    "                torch.nn.Flatten(0,1)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "              #x=self.flatten(x)\n",
    "              logits = self.linear_relu_stack(x)\n",
    "              return logits\n",
    "\n",
    "class CosNetwork3(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.linear_relu_stack = nn.Sequential(\n",
    "                nn.Linear(1, 20),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(20, 20),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(20, 20),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(20, 14),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(14, 12),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(12, 10),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(10, 10),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(10,10),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(10, 9),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(9, 1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1,1),\n",
    "                torch.nn.Flatten(0,1)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "              logits = self.linear_relu_stack(x)\n",
    "              return logits\n",
    "        \n",
    "\n",
    "cos_model1, cos_model2, cos_model3 = CosNetwork(), CosNetwork2(), CosNetwork3()\n",
    "\n",
    "display_model_info(\"cos_model1\", cos_model1)\n",
    "display_model_info(\"cos_model2\", cos_model2)\n",
    "display_model_info(\"cos_model3\", cos_model3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2619941234588623\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.333909 \n",
      "\n",
      "100 0.03207455575466156\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.006065 \n",
      "\n",
      "200 0.027504129335284233\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.005125 \n",
      "\n",
      "300 0.02478814125061035\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.004468 \n",
      "\n",
      "400 0.02271406538784504\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.003945 \n",
      "\n",
      "500 0.020939311012625694\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.003526 \n",
      "\n",
      "600 0.019347041845321655\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.003173 \n",
      "\n",
      "700 0.017900144681334496\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.002870 \n",
      "\n",
      "800 0.016577979549765587\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.002567 \n",
      "\n",
      "900 0.015359866432845592\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.002332 \n",
      "\n",
      "1000 0.014240951277315617\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.002120 \n",
      "\n",
      "1100 0.013217606581747532\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.001929 \n",
      "\n",
      "1200 0.012262754142284393\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.001756 \n",
      "\n",
      "1300 0.011362282559275627\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.001603 \n",
      "\n",
      "1400 0.010573683306574821\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.001177 \n",
      "\n",
      "1500 0.009777302853763103\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.001082 \n",
      "\n",
      "1600 0.009139658883213997\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.000779 \n",
      "\n",
      "1700 0.008526524528861046\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.000724 \n",
      "\n",
      "1800 0.00800085999071598\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.000674 \n",
      "\n",
      "1900 0.007530029863119125\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.000629 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.linspace(-math.pi, math.pi, 5000, dtype=dtype)\n",
    "y = torch.cos(x)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "epochs = 2000\n",
    "lr = 1e-3\n",
    "batch_size=100\n",
    "optimizer1 = torch.optim.SGD(cos_model1.parameters(), lr=lr)\n",
    "train_dataloader = DataLoader(TensorDataset(x.unsqueeze(1),y.unsqueeze(1)), batch_size=batch_size)\n",
    "val_dataloader = DataLoader(TensorDataset(x.unsqueeze(1),y.unsqueeze(1)), batch_size=batch_size)\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "display(\"Training & eval: cos1 model\")\n",
    "for t in range(epochs):\n",
    "    train_loop(train_dataloader, cos_model1, loss_fn, optimizer1, t)\n",
    "    val_loss = val_loop(val_dataloader, cos_model1, loss_fn, t)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement +=1\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        display(f'Convergence reached at {epoch}')\n",
    "        break\n",
    "display(\"Done\")\n",
    "\n",
    "optimizer2 = torch.optim.SGD(cos_model2.parameters(), lr=lr)\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "display(\"Training & eval: cos2 model\")\n",
    "for t in range(epochs):\n",
    "    train_loop(train_dataloader, cos_model2, loss_fn, optimizer2, t)\n",
    "    val_loss = val_loop(val_dataloader, cos_model2, loss_fn, t)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement +=1\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        display(f'Convergence reached at {epoch}')\n",
    "        break\n",
    "display(\"Done\")\n",
    "\n",
    "optimizer3 = torch.optim.SGD(cos_model3.parameters(), lr=lr)\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "display(\"Training & eval: cos3 model\")\n",
    "for t in range(epochs):\n",
    "    train_loop(train_dataloader, cos_model3, loss_fn, optimizer3, t)\n",
    "    val_loss = val_loop(val_dataloader, cos_model3, loss_fn, t)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement +=1\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        display(f'Convergence reached at {epoch}')\n",
    "        break\n",
    "display(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
