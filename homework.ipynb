{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN - Dense layers\n",
    "CNN - Convolutional neural network\n",
    "\n",
    "# 1-1 - \n",
    "## rank of input x from 0 to 2pi. \n",
    "Do not need to set the # of parameters equal, just close.\n",
    "\n",
    "First figure is training loss\n",
    "2nd figure is prediction of each model\n",
    "\n",
    "Import MNIST or CIFAR-10 from Torch vision. Use either CNN or DNN. Show loss and accuracy. Comment on results & show observservations\n",
    "\n",
    "# 1-2\n",
    "- Collect parameters of the models - Grab from every layer - Collect and put into 1 dimensional feaure than reduce to a feature with only 2 numbers\n",
    "- Look up PCA\n",
    "- FIgiure out how to compute second order optimization method ex: Newton's method or Levenberg-Marquardt algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1 Training on a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports of Pytorch and matplotlib and other supporting modules\n",
    " \n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting default device\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "f'{device} is available'\n",
    "\n",
    "dtype = torch.float\n",
    "torch.set_default_device(device)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimize, epoch):\n",
    "    \"\"\"Training loop funciton for non-linear function\"\"\"\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        y_pred = model(X)\n",
    "        y_pred = y_pred.unsqueeze(1)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimize.step()\n",
    "        optimize.zero_grad()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if epoch % 100 == 0:\n",
    "            print(epoch, loss.item())\n",
    "\n",
    "def val_loop(dataloader, model, loss_fn, epoch):\n",
    "    \"\"\"Eval loop function for non-linear function\"\"\"\n",
    "    model.eval()\n",
    "    size= len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0,0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            y_pred = model(X)\n",
    "            y_pred = y_pred.unsqueeze(1)\n",
    "            test_loss += loss_fn(y_pred, y).item()\n",
    "            correct +=(y_pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /=size\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        display(f\"Accuracy: {(100*correct)}%, Avg loss: {test_loss}\")\n",
    "\n",
    "    return test_loss\n",
    "\n",
    "def cnt_model_params(model):\n",
    "    \"\"\"Count model parameters\"\"\"\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            count+=param.numel()\n",
    "    return count\n",
    "\n",
    "def display_model_info(model_name, model):\n",
    "    \"\"\" Display model information\"\"\"\n",
    "    count = 0\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Module):\n",
    "            count+=1\n",
    "    display(model)\n",
    "    display(f\"{model_name}. parameters: {cnt_model_params(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosNetwork(nn.Module):\n",
    "        \"\"\"First DNN for Cosine function\"\"\"\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.linear_relu_stack = nn.Sequential(\n",
    "                nn.Linear(1, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 12),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(12, 1),\n",
    "                torch.nn.Flatten(0,1)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "              logits = self.linear_relu_stack(x)\n",
    "              return logits\n",
    "        \n",
    "class CosNetwork2(nn.Module):\n",
    "        \"\"\"second DNN for Cosine function\"\"\"\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.linear_relu_stack = nn.Sequential(\n",
    "                nn.Linear(1, 453),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(453, 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(2,1),\n",
    "                torch.nn.Flatten(0,1)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "              #x=self.flatten(x)\n",
    "              logits = self.linear_relu_stack(x)\n",
    "              return logits\n",
    "\n",
    "class CosNetwork3(nn.Module):\n",
    "        \"\"\"Third DNN for Cosine function\"\"\"\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.linear_relu_stack = nn.Sequential(\n",
    "                nn.Linear(1, 20),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(20, 20),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(20, 20),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(20, 14),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(14, 12),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(12, 10),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(10, 10),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(10,10),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(10, 9),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(9, 1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1,1),\n",
    "                torch.nn.Flatten(0,1)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "              logits = self.linear_relu_stack(x)\n",
    "              return logits\n",
    "        \n",
    "\n",
    "cos_model1, cos_model2, cos_model3 = CosNetwork(), CosNetwork2(), CosNetwork3()\n",
    "\n",
    "display_model_info(\"cos_model1\", cos_model1)\n",
    "display_model_info(\"cos_model2\", cos_model2)\n",
    "display_model_info(\"cos_model3\", cos_model3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-math.pi, math.pi, 5000, dtype=dtype)\n",
    "y = torch.cos(x)\n",
    "\n",
    "lossy1, lossy2, lossy3 = list(), list(), list()\n",
    "epochx1, epochx2, epochx3 = list(), list(), list()\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "epochs = 20001\n",
    "lr = 1e-3\n",
    "batch_size=100\n",
    "optimizer1 = torch.optim.SGD(cos_model1.parameters(), lr=lr)\n",
    "train_dataloader = DataLoader(TensorDataset(x.unsqueeze(1),y.unsqueeze(1)), batch_size=batch_size)\n",
    "val_dataloader = DataLoader(TensorDataset(x.unsqueeze(1),y.unsqueeze(1)), batch_size=batch_size)\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "display(\"Training & eval: cos1 model\")\n",
    "for epoch in range(epochs):\n",
    "    train_loop(train_dataloader, cos_model1, loss_fn, optimizer1, epoch)\n",
    "    val_loss = val_loop(val_dataloader, cos_model1, loss_fn, epoch)\n",
    "\n",
    "    if epoch%100 == 0:\n",
    "        epochx1.append(epoch)\n",
    "        lossy1.append(val_loss)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement +=1\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        display(f'Convergence reached at {epoch}')\n",
    "        break\n",
    "display(\"Done\")\n",
    "\n",
    "optimizer2 = torch.optim.SGD(cos_model2.parameters(), lr=lr)\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "display(\"Training & eval: cos2 model\")\n",
    "for epoch in range(epochs):\n",
    "    train_loop(train_dataloader, cos_model2, loss_fn, optimizer2, epoch)\n",
    "    val_loss = val_loop(val_dataloader, cos_model2, loss_fn, epoch)\n",
    "\n",
    "    if epoch%100 == 0:\n",
    "        epochx2.append(epoch)\n",
    "        lossy2.append(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement +=1\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        display(f'Convergence reached at {epoch}')\n",
    "        break\n",
    "display(\"Done\")\n",
    "\n",
    "optimizer3 = torch.optim.SGD(cos_model3.parameters(), lr=lr)\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "display(\"Training & eval: cos3 model\")\n",
    "for epoch in range(epochs):\n",
    "    train_loop(train_dataloader, cos_model3, loss_fn, optimizer3, epoch)\n",
    "    val_loss = val_loop(val_dataloader, cos_model3, loss_fn, epoch)\n",
    "\n",
    "    if epoch%100 == 0:\n",
    "        epochx3.append(epoch)\n",
    "        lossy3.append(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement +=1\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        display(f'Convergence reached at {epoch}')\n",
    "        break\n",
    "display(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting model loss and ground truth for cosine neural networks\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochx1, lossy1, epochx2, lossy2, epochx3, lossy3)\n",
    "ax.set(xlabel=\"epochs\",ylabel=\"loss\", title=\"Model loss\")\n",
    "ax.legend(labels=['cos_model1','cos_model2','cos_model3'])\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "x = torch.linspace(-math.pi, math.pi, 5000, dtype=dtype)\n",
    "y = torch.cos(x)\n",
    "\n",
    "with torch.no_grad():\n",
    "    m1_y = cos_model1(x.unsqueeze(1)).cpu().numpy()\n",
    "    m2_y = cos_model2(x.unsqueeze(1)).cpu().numpy()\n",
    "    m3_y = cos_model3(x.unsqueeze(1)).cpu().numpy()\n",
    "    x = x.cpu().numpy()\n",
    "    y = y.cpu().numpy()\n",
    "\n",
    "ax2.plot(x, m1_y, x, m2_y, x, m3_y, x, y)\n",
    "ax2.set(xlabel='x',ylabel='y', title='Ground truth')\n",
    "ax2.legend(labels=['cos_model1', 'cos_model2','cos_model3', 'Ground Truth'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. Training on actual task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"print(len(training_ds))\\nprint(len(test_ds))\\nimg, lbl = test_ds[9]\\nprint(lbl)\\nplt.imshow(img.squeeze(), cmap='gray')\\nimg.shape\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "training_ds = datasets.MNIST(root=\"data\", train=True, download=True, transform=ToTensor())\n",
    "test_ds = datasets.MNIST(root='data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "\"\"\"print(len(training_ds))\n",
    "print(len(test_ds))\n",
    "img, lbl = test_ds[9]\n",
    "print(lbl)\n",
    "plt.imshow(img.squeeze(), cmap='gray')\n",
    "img.shape\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMISTNetwork1(\n",
       "  (seq): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Flatten(start_dim=1, end_dim=-1)\n",
       "    (3): Linear(in_features=18432, out_features=128, bias=True)\n",
       "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
       "    (5): Softmax(dim=0)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'nmist1. parameters: 2361546'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "NMISTNetwork2(\n",
       "  (seq): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Flatten(start_dim=1, end_dim=-1)\n",
       "    (5): Linear(in_features=25600, out_features=128, bias=True)\n",
       "    (6): Linear(in_features=128, out_features=10, bias=True)\n",
       "    (7): Softmax(dim=0)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'nmist2. parameters: 3330314'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "NMISTNetwork3(\n",
       "  (seq): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (3): Dropout(p=0.25, inplace=False)\n",
       "    (4): ReLU()\n",
       "    (5): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    (7): Linear(in_features=16384, out_features=128, bias=True)\n",
       "    (8): ReLU()\n",
       "    (9): Linear(in_features=128, out_features=10, bias=True)\n",
       "    (10): Softmax(dim=0)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'nmist3. parameters: 2253130'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class NMISTNetwork1(nn.Module):\n",
    "        \"\"\"First CNN for NMIST\"\"\"\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.seq = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5), \n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(1), \n",
    "            nn.Linear(18432, 128),\n",
    "            nn.Linear(128, 10),\n",
    "            nn.Softmax(0)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "              output = self.seq(x)\n",
    "              return output\n",
    "        \n",
    "class NMISTNetwork2(nn.Module):\n",
    "        \"\"\"Second CNN for NMIST\"\"\"\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.seq = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(1), \n",
    "            nn.Linear(25600, 128),\n",
    "            nn.Linear(128, 10),\n",
    "            nn.Softmax(0)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "              output = self.seq(x)\n",
    "              return output\n",
    "        \n",
    "class NMISTNetwork3(nn.Module):\n",
    "        \"\"\"Third CNN for NMIST\"\"\"\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.seq = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 5),\n",
    "            nn.Dropout(.25),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 5),\n",
    "            nn.Flatten(1), \n",
    "            nn.Linear(16384, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "            nn.Softmax(0)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "              output = self.seq(x)\n",
    "              return output\n",
    "        \n",
    "\n",
    "nmist1, nmist2, nmist3 = NMISTNetwork1(), NMISTNetwork2(), NMISTNetwork3()\n",
    "display_model_info('nmist1', nmist1)\n",
    "display_model_info('nmist2', nmist2)\n",
    "display_model_info('nmist3', nmist3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmist_train_loop(dataloader, model, loss_fn, optimize, epoch):\n",
    "    \"\"\"Training loop funciton for non-linear function\"\"\"\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        output = model(X)\n",
    "        y_pred = torch.max(output.data, 1).indices\n",
    "        display(y_pred, y)\n",
    "        # loss = loss_fn(y_pred, y)\n",
    "        loss = nn.functional.nll_loss(y_pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimize.step()\n",
    "        optimize.zero_grad()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if epoch % 10 == 0:\n",
    "            print(epoch, loss.item())\n",
    "\n",
    "def nmist_val_loop(dataloader, model, loss_fn, epoch):\n",
    "    \"\"\"Eval loop function for non-linear function\"\"\"\n",
    "    model.eval()\n",
    "    size= len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0,0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            X = X.to(device)\n",
    "            output = model(X)\n",
    "            y_pred = torch.max(output, 1).indices\n",
    "            test_loss += loss_fn(y_pred, y).item()\n",
    "            correct +=(y_pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /=size\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        display(f\"Accuracy: {(100*correct)}%, Avg loss: {test_loss}\")\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training & eval: nmist1 model'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 0, 1, 8, 0, 9, 5, 9, 4, 6, 4, 9, 9, 9, 4, 9, 0, 2, 3, 6, 1, 8, 1,\n",
       "        2, 0, 9, 7, 3, 1, 7, 5], device='mps:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
       "        1, 2, 4, 3, 2, 7, 3, 8], device='mps:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "lossy1, lossy2, lossy3 = list(), list(), list()\n",
    "epochx1, epochx2, epochx3 = list(), list(), list()\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 75\n",
    "lr = 1e-3\n",
    "batch_size=32\n",
    "\n",
    "training_dl = DataLoader(training_ds, batch_size=batch_size)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "optimizer1 = torch.optim.SGD(nmist1.parameters(), lr=lr)\n",
    "display(\"Training & eval: nmist1 model\")\n",
    "for epoch in range(epochs):\n",
    "    nmist_train_loop(training_dl, nmist1, loss_fn, optimizer1, epoch)\n",
    "    val_loss = nmist_val_loop(test_dl, nmist1, loss_fn, epoch)\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "        epochx1.append(epoch)\n",
    "        lossy1.append(val_loss)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement +=1\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        display(f'Convergence reached at {epoch}')\n",
    "        break\n",
    "display(\"Done\")\n",
    "\n",
    "optimizer2 = torch.optim.SGD(nmist2.parameters(), lr=lr)\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "display(\"Training & eval: nmist2 model\")\n",
    "for epoch in range(epochs):\n",
    "    nmist_train_loop(training_dl, nmist2, loss_fn, optimizer2, epoch)\n",
    "    val_loss = nmist_val_loop(test_dl, nmist2, loss_fn, epoch)\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "        epochx2.append(epoch)\n",
    "        lossy2.append(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement +=1\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        display(f'Convergence reached at {epoch}')\n",
    "        break\n",
    "display(\"Done\")\n",
    "\n",
    "optimizer3 = torch.optim.SGD(nmist3.parameters(), lr=lr)\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "display(\"Training & eval: nmist3 model\")\n",
    "for epoch in range(epochs):\n",
    "    nmist_train_loop(training_dl, nmist3, loss_fn, optimizer3, epoch)\n",
    "    val_loss = nmist_val_loop(test_dl, nmist3, loss_fn, epoch)\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "        epochx3.append(epoch)\n",
    "        lossy3.append(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement +=1\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        display(f'Convergence reached at {epoch}')\n",
    "        break\n",
    "display(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       " tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
       "         1, 2, 4, 3, 2, 7, 3, 8], device='mps:0')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(training_dl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 0, 1], device='mps:0')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nmist1(torch.rand([1, 1,28,28]))\n",
    "\n",
    "val = torch.rand(32, 1, 28, 28)\n",
    "# torch.nn.functional.softmax(val, dim=0).shape\n",
    "torch.max(nmist1(torch.rand((3,1,28,28))), 1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
